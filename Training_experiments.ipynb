{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport rasterio\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport cv2\nimport os\n# os.environ['CUDA_LAUNCH_BLOCKING']='1'\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-25T14:13:16.453617Z","iopub.execute_input":"2023-07-25T14:13:16.454003Z","iopub.status.idle":"2023-07-25T14:13:17.149571Z","shell.execute_reply.started":"2023-07-25T14:13:16.453974Z","shell.execute_reply":"2023-07-25T14:13:17.148261Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Final Approach: Mask2Former","metadata":{}},{"cell_type":"code","source":"pip install -U accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n# import torch.multiprocessing as mp\n# mp.set_start_method('spawn', force=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T14:13:34.136777Z","iopub.execute_input":"2023-07-25T14:13:34.137285Z","iopub.status.idle":"2023-07-25T14:13:38.062858Z","shell.execute_reply.started":"2023-07-25T14:13:34.137226Z","shell.execute_reply":"2023-07-25T14:13:38.061862Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install -U pytorch_lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import Mask2FormerImageProcessor#, Mask2FormerConfig\ntype_dict = {0:'blood_vessel', 1:'glomerulus', 2:'unsure'}\n\n# load Mask2Former fine-tuned on COCO instance segmentation\n\n# model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-instance\", \n#                                                             id2label=type_dict,\n#                                                             ignore_mismatched_sizes=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-25T14:13:38.064380Z","iopub.execute_input":"2023-07-25T14:13:38.065698Z","iopub.status.idle":"2023-07-25T14:13:49.745480Z","shell.execute_reply.started":"2023-07-25T14:13:38.065639Z","shell.execute_reply":"2023-07-25T14:13:49.744611Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache() \nimport gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processor = Mask2FormerImageProcessor(reduce_labels=True,\n#                                                       ignore_index=255,\n#                                                       do_resize=False, \n#                                                       do_rescale=False, \n#                                                       do_normalize=False)\n# configuration = Mask2FormerConfig(id2label=type_dict)\n# model = Mask2FormerForUniversalSegmentation(configuration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv(\"/kaggle/input/hubmap-train-test/train_set.csv\")\n# val = pd.read_csv(\"/kaggle/input/hubmap-train-test/val_set.csv\")\n# train_gr = train.groupby('0').agg(lambda x: list(x))\n# val_gr = val.groupby('0').agg(lambda x: list(x))\ntrain_gr = pd.read_csv(\"/kaggle/input/hubmap-train-test/train_grouped.csv\")\nval_gr = pd.read_csv(\"/kaggle/input/hubmap-train-test/val_grouped.csv\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCode to visualize images\n\"\"\"\n# import ast\n# item = train_gr.iloc[0] \n# image = rasterio.open(item.path).read()\n# type_ = ast.literal_eval(item['structure_types'])\n# coords = ast.literal_eval(item['coordinates'])\n# image_ = torch.from_numpy(image).float() / 255.0  # Normalize image to [0, 1]\n# image_shape = image.shape[1:]\n# mask = np.zeros(image_shape, dtype=np.uint8)\n# pts_array = []\n# obj_cnt = 1\n# type_dict = {'blood_vessel':1, 'glomerulus':2, 'unsure':3}\n# inst2cls ={0:0}\n# print(type(coords))\n# for typ, coord in zip(type_, coords):\n#     pts = np.array(coord).reshape((-1, 1, 2)).astype(np.int32)\n# #     print(pts)\n#     cv2.fillPoly(mask, [pts], obj_cnt)\n#     inst2cls[obj_cnt] = type_dict[typ]\n#     obj_cnt+=1\n# #     break\n# image = image.transpose(1,2,0)\n# #     print()\n# #     continue\n# arr = [0,0,0]\n# # Overlay the mask on the image\n# overlay = np.copy(image)\n# print(arr)\n# overlay[mask > 0] =  (1,0,0)# Set the mask pixels to red (adjust color as needed)\n\n# # Plot the image with the overlay\n# fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n# axes[0].imshow(image)\n# axes[0].set_title('Image')\n# axes[1].imshow(mask)\n# axes[1].set_title('Image with Mask Overlay')\n# axes[2].imshow(overlay)\n# axes[2].set_title('Image with Mask Overlay')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segmentation_map = np.where(mask == 0, 255, mask - 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inputs = processor([image], [mask], instance_id_to_semantic_id=inst2cls, return_tensors=\"pt\")\n# inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nPytorch Datasets\n\n\"\"\"\nimport ast\nclass CustomDataset_instances:\n    \"\"\"\n    Instance-wise Segmentation\n    \"\"\"\n    def __init__(self, df, transform):\n        self.df = df\n        self.processor = Mask2FormerImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-instance\",\n                                                      reduce_labels=True,\n                                                      ignore_index=255,\n                                                      do_resize=False, \n                                                      do_rescale=False, \n                                                      do_normalize=False)\n        self.transform = transform\n    def __getitem__(self, index):\n        # Load image\n        item = self.df.iloc[index]\n        image = rasterio.open(item.path).read()\n        type_ = ast.literal_eval(item['structure_types'])\n        coords = ast.literal_eval(item['coordinates'])\n#         image = image.astype(float) / 255.0  # Normalize image to [0, 1]\n        image_shape = image.shape[1:]\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        pts_array = []\n        obj_cnt = 1\n        type_dict = {'blood_vessel':0, 'glomerulus':1, 'unsure':2}\n        inst2cls = {0:0}\n        for typ, coord in zip(type_, coords):\n            if typ == 'unsure':\n                continue\n            else:\n                pts = np.array(coord).reshape((-1, 1, 2)).astype(np.int32)\n                cv2.fillPoly(mask, [pts], obj_cnt)\n                inst2cls[obj_cnt] = type_dict[typ]+1\n                obj_cnt+=1\n        if self.transform is not None:\n            transformed = self.transform(image=image.transpose(1,2,0), mask=mask)\n            image, mask = transformed['image'], transformed['mask']\n            image = image.transpose(2,0,1)\n        if len(inst2cls.keys())==1:\n            # Some image does not have annotation (all ignored)\n            inputs = self.processor([image], return_tensors=\"pt\")\n            inputs = {k:v.squeeze() for k,v in inputs.items()}\n            inputs[\"class_labels\"] = torch.tensor([3])\n            inputs[\"mask_labels\"] = torch.zeros((0, inputs[\"pixel_values\"].shape[-2], inputs[\"pixel_values\"].shape[-1]))\n            return inputs\n        inputs = self.processor([image], [mask], instance_id_to_semantic_id=inst2cls, return_tensors=\"pt\", reduce_labels=True)\n        inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()}\n#         print(inputs['class_labels'])\n        return inputs\n\n    def __len__(self):\n        return len(self.df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\nADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\nADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n\n# note that you can include more fancy data augmentation methods here\ntrain_transform = A.Compose([\n    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n    class_labels = [example[\"class_labels\"] for example in batch]\n    mask_labels = [example[\"mask_labels\"] for example in batch]\n    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import Trainer, TrainingArguments\n# training_args = TrainingArguments(\n#     output_dir='./results',          # output directory\n#     num_train_epochs=3,              # total number of training epochs\n#     per_device_train_batch_size=16,  # batch size per device during training\n#     per_device_eval_batch_size=64,   # batch size for evaluation\n#     warmup_steps=100,                # number of warmup steps for learning rate scheduler\n#     weight_decay=0.01,               # strength of weight decay\n#     logging_dir='./logs',            # directory for storing logs\n#     logging_steps=10,\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset_instances(train_gr, train_transform)\nval_dataset = CustomDataset_instances(val_gr, train_transform)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the dataloaders once","metadata":{}},{"cell_type":"code","source":"samp_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item = next(iter(samp_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = item['pixel_mask'][1].numpy()\nplt.imshow(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = item['mask_labels'][1].numpy()\nplt.imshow(im[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item['class_labels']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item['pixel_mask'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item['mask_labels'][0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\noutput = model(pixel_values = item['pixel_values'],\n                mask_labels = item[\"mask_labels\"],\n                class_labels = item[\"class_labels\"],\n     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cudnn.enabled = False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training pytorch_lightning","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\nclass Mask2Former(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        from transformers import Mask2FormerForUniversalSegmentation\n        self.type_dict = {0:'blood_vessel', 1:'glomerulus', 2:'unsure'}\n        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-instance\", \n                                                            id2label=self.type_dict,\n                                                            ignore_mismatched_sizes=True)\n\n        self.learning_rate = 5e-5\n\n    def forward(self, pixel_values, mask_labels, class_labels):\n        return self.model(pixel_values, mask_labels, class_labels)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            pixel_values=batch[\"pixel_values\"],\n            mask_labels=[labels for labels in batch[\"mask_labels\"]],\n            class_labels=[labels for labels in batch[\"class_labels\"]],\n        )\n        loss = outputs.loss\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(\n            pixel_values=batch[\"pixel_values\"],\n            mask_labels=[labels for labels in batch[\"mask_labels\"]],\n            class_labels=[labels for labels in batch[\"class_labels\"]],\n        )\n        val_loss = outputs.loss\n        self.log('val_loss', val_loss, on_step=False, on_epoch=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=0.2)\n\n        lr_scheduler_cont = {\n            'scheduler':  torch.optim.lr_scheduler.ReduceLROnPlateau(\n                                                                        optimizer,\n                                                                        mode='min',\n                                                                        factor=0.8,\n                                                                        patience=50,\n                                                                        threshold=1e-2,\n                                                                        min_lr=1e-8,\n                                                                        verbose=True\n                                                                    ),\n            'name': 'learning_rate',\n            'monitor' : 'train_loss',\n            \"interval\" : \"step\" \n        }\n        return optimizer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_m2f = Mask2Former()\nwandb_logger = WandbLogger()\ncheckpoint_callback = ModelCheckpoint(dirpath=\"/kaggle/working/ckpt/\", save_top_k=3, monitor=\"val_loss\")\ntrainer = pl.Trainer(max_epochs=50, devices='auto',accelerator=\"gpu\",precision=16, logger=wandb_logger, callbacks=[checkpoint_callback])  # Set the appropriate number of GPUs\ntrainer.fit(model_m2f, train_dataloader, val_dataloader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nfrom pytorch_lightning.callbacks import LearningRateMonitor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mask2Former_pred(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        from transformers import Mask2FormerForUniversalSegmentation\n        self.type_dict = {0:'blood_vessel', 1:'glomerulus', 2:'unsure'}\n        self.model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-instance\", \n                                                            id2label=self.type_dict,\n                                                            ignore_mismatched_sizes=True)\n\n        self.learning_rate = 5e-5\n\n    def forward(self, pixel_values, mask_labels, class_labels):\n        return self.model(pixel_values, mask_labels, class_labels)\n\n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            pixel_values=batch[\"pixel_values\"],\n            mask_labels=[labels for labels in batch[\"mask_labels\"]],\n            class_labels=[labels for labels in batch[\"class_labels\"]],\n        )\n        loss = outputs.loss\n        self.log('train_loss', loss, on_step=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(\n            pixel_values=batch[\"pixel_values\"],\n            mask_labels=[labels for labels in batch[\"mask_labels\"]],\n            class_labels=[labels for labels in batch[\"class_labels\"]],\n        )\n        val_loss = outputs.loss\n        self.log('val_loss', val_loss, on_step=False, on_epoch=True)\n        \n    def predict_step(self, batch, batch_idx):\n        print(batch)\n        outputs = self.model(\n            pixel_values=batch[\"pixel_values\"],\n        )\n        return outputs\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=0.2)\n\n        lr_scheduler_cont = {\n            'scheduler':  torch.optim.lr_scheduler.ReduceLROnPlateau(\n                                                                        optimizer,\n                                                                        mode='min',\n                                                                        factor=0.8,\n                                                                        patience=50,\n                                                                        threshold=1e-2,\n                                                                        min_lr=1e-8,\n                                                                        verbose=True\n                                                                    ),\n            'name': 'learning_rate',\n            'monitor' : 'train_loss',\n            \"interval\" : \"step\" \n        }\n        return optimizer\n\n\n\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type_dict = {0:'blood_vessel', 1:'glomerulus', 2:'unsure'}\nfrom transformers import Mask2FormerForUniversalSegmentation\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-instance\", \n                                                            id2label=type_dict,\n                                                            ignore_mismatched_sizes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\nclass CustomDataset_pred:\n    def __init__(self, df, transform):\n        self.df = df\n        self.processor = Mask2FormerImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-cityscapes-instance\",\n                                                      reduce_labels=True,\n                                                      ignore_index=255,\n                                                      do_resize=False, \n                                                      do_rescale=False, \n                                                      do_normalize=False)\n        self.transform = transform\n    def __getitem__(self, index):\n        # Load image\n        item = self.df[index]\n        image = rasterio.open(item).read()\n        \n        if self.transform is not None:\n            transformed = self.transform(image=image.transpose(1,2,0))\n            image= transformed['image']\n            image = image.transpose(2,0,1)\n            \n        inputs = self.processor([image], return_tensors=\"pt\",reduce_labels=True)\n        inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()}\n        return inputs\n\n    def __len__(self):\n        return len(self.df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dataset = CustomDataset_pred(['/kaggle/input/hubmap-hacking-the-human-vasculature/test/72e40acccadf.tif'], train_transform)\nsamp_dataloader = torch.utils.data.DataLoader(pred_dataset, batch_size=1, shuffle=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chk = torch.load(\"/kaggle/working/ckpt/epoch=14-step=2460.ckpt\", map_location=torch.device('cpu'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.load_state_dict(chk[\"state_dict\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pred = Mask2Former_pred.load_from_checkpoint(\"/kaggle/working/ckpt/epoch=14-step=2460.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_pred = pl.Trainer(devices='auto',accelerator=\"cpu\",precision=16)  # Set the appropriate number of GPUs\n\nres = trainer_pred.predict(model_pred, samp_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_instance_map = processor.post_process_instance_segmentation(\n    res[0], target_sizes=[(512,512)], return_binary_maps=True\n\n)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(pred_instance_map[0]['segmentation'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(pred_instance_map['segmentation'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img= cv2.cvtColor(cv2.imread('/kaggle/input/hubmap-hacking-the-human-vasculature/test/72e40acccadf.tif'), cv2.COLOR_BGR2RGB) #return np array of pixels \nplt.figure()\nplt.imshow(img)\nplt.title(\"Image\")\nplt.show()\n\n# predicted_semantic_map = processor.post_process_instance_segmentation(outputs, target_sizes=[(512,512)])[0]\n# color_segmentation_map = pred_instance_map['segmentation']\noverlay = np.zeros([512,512])\nfor d in pred_instance_map['segments_info']:\n    print(\"hi\")\n    if d['score']>0.7:\n        plt.imshow(pred_instance_map['segmentation'][d['id']])\n        \n        plt.show()\n        overlay+=pred_instance_map['segmentation'][d['id']].numpy()\n#         color_segmentation_map[color_segmentation_map==d['id']] = 50+50*d['label_id']\n#         overlay[color_segmentation_map==d['id']] = 50+50*d['label_id']\n#         plt.imshow(overlay)\n        \n#     else:\n#         color_segmentation_map[color_segmentation_map==d['id']] = 0\n#     plt.imshow(color_segmentation_map, alpha=0.6)\n# #     color_segmentation_map = color_segmentation_map.abs()\n# #     print(color_segmentation_map)\nplt.imshow(img.reshape(512,512,3))\nplt.imshow(overlay, alpha=0.6)\n# plt.title(\"Predictions\")\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overlay.shape, pred_instance_map['segmentation'][d['id']].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    outputs = model_m2f.model(pixel_values = torch.from_numpy(np.array(inputs['pixel_values'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_queries_logits = outputs.class_queries_logits\nmasks_queries_logits = outputs.masks_queries_logits\n\n# Perform post-processing to get instance segmentation map\npred_instance_map = processor.post_process_semantic_segmentation(\n    outputs, target_sizes=[(512,512)]\n\n)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_instance_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image.transpose(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(pred_instance_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Manual Training loop","metadata":{}},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\nrunning_loss = 0.0\nnum_samples = 0\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    print(\"Epoch:\", epoch)\n    running_loss = 0.0\n    pbar = tqdm(train_dataloader)\n    model.train()\n    for idx, batch in enumerate(pbar):\n        # Reset the parameter gradients\n        optimizer.zero_grad()\n#         print(batch[\"class_labels\"])\n        # Forward pass\n        outputs = model(\n                pixel_values=batch[\"pixel_values\"].cuda(),\n#                 pixel_mask = batch[\"pixel_mask\"].cuda(),\n                mask_labels=[labels.cuda() for labels in batch[\"mask_labels\"]],\n                class_labels=[labels.cuda() for labels in batch[\"class_labels\"]],\n        )\n\n        # Backward propagation\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        \n        pbar.set_postfix({'loss':(running_loss/(idx+1))})\n        \n    # Print training loss for the epoch\n    epoch_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n\n    # Evaluation on the validation set\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in tqdm(val_dataloader):\n#             images = batch['image']\n            outputs = model(\n                    pixel_values=batch[\"pixel_values\"].cuda(),\n#                     pixel_mask = batch[\"pixel_mask\"].cuda(),\n                    mask_labels=[labels.cuda() for labels in batch[\"mask_labels\"]],\n                    class_labels=[labels.cuda() for labels in batch[\"class_labels\"]],\n            )\n\n            # Calculate the loss\n            loss = outputs.loss\n            val_loss += loss.item()\n            \n    # Print validation loss for the epoch\n    val_loss /= len(val_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\nmodel\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0025)\n\nrunning_loss = 0.0\nnum_samples = 0\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    print(\"Epoch:\", epoch)\n    running_loss = 0.0\n    pbar = tqdm(train_dataloader)\n    model.train()\n    for idx, batch in enumerate(pbar):\n        # Reset the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(\n                pixel_values=batch[\"pixel_values\"],\n#                 pixel_mask = batch[\"pixel_mask\"].cuda(),\n                mask_labels=[labels for labels in batch[\"mask_labels\"]],\n                class_labels=[labels for labels in batch[\"class_labels\"]],\n        )\n\n        # Backward propagation\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        \n        pbar.set_postfix({'loss':(running_loss/(idx+1))})\n        \n    # Print training loss for the epoch\n    epoch_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n\n    # Evaluation on the validation set\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in tqdm(val_dataloader):\n#             images = batch['image']\n            outputs = model(\n                    pixel_values=batch[\"pixel_values\"],\n#                     pixel_mask = batch[\"pixel_mask\"].cuda(),\n                    mask_labels=[labels for labels in batch[\"mask_labels\"]],\n                    class_labels=[labels for labels in batch[\"class_labels\"]],\n            )\n\n            # Calculate the loss\n            loss = outputs.loss\n            val_loss += loss.item()\n            \n    # Print validation loss for the epoch\n    val_loss /= len(val_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dataloader))\nfor k,v in batch.items():\n    if isinstance(v, torch.Tensor):\n        print(k,v.shape)\n    else:\n        print(k,len(v))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([label.shape for label in batch[\"class_labels\"]])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([label.shape for label in batch[\"mask_labels\"]])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(\n        pixel_values=batch[\"pixel_values\"].cuda(),\n        pixel_mask = batch[\"pixel_mask\"].cuda(),\n        mask_labels=[labels.cuda() for labels in batch[\"mask_labels\"]],\n        class_labels=[labels.cuda() for labels in batch[\"class_labels\"]],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.train()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Previous failed approaches","metadata":{}},{"cell_type":"code","source":"import torch\nTORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\nCUDA_VERSION = torch.__version__.split(\"+\")[-1]\nprint(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n# Install detectron2 that matches the above pytorch version\n# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clone and install Mask2Former\n!git clone https://github.com/facebookresearch/Mask2Former.git\n%cd Mask2Former\n!pip install -U opencv-python\n!pip install git+https://github.com/cocodataset/panopticapi.git\n!pip install -r requirements.txt\n%cd mask2former/modeling/pixel_decoder/ops\n!python setup.py build install\n%cd ../../../../","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation = mmengine.load('./ballondatasets/balloon/train/via_region_data.json')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openmim\n!mim install git+https://github.com/open-mmlab/mmengine.git\n!mim install git+https://github.com/open-mmlab/mmcv.git\n!mim install git+https://github.com/open-mmlab/mmdetection.git\n!pip install -U numpy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -qqq /kaggle/input/mmdetectron-31-wheel/*.whl\n!pip install -U numpy\n# !pip install git+https://github.com/open-mmlab/mmengine.git\n# !pip install git+https://github.com/open-mmlab/mmcv.git\n# !pip install git+https://github.com/open-mmlab/mmdetection.git\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Pytorch installation\nimport torch, torchvision\nprint(\"torch version:\",torch.__version__, \"cuda:\",torch.cuda.is_available())\n\n# Check MMDetection installation\nimport mmdet\nprint(\"mmdetection:\",mmdet.__version__)\n\n# Check mmcv installation\nimport mmcv\nprint(\"mmcv:\",mmcv.__version__)\n\n# Check mmengine installation\nimport mmengine\nprint(\"mmengine:\",mmengine.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q git+https://github.com/qubvel/segmentation_models.pytorch\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q pycocotools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ../\n!zip -r all_req.zip wheels\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Load the image data\n# image =  rasterio.open(\"/kaggle/input/hubmap-hacking-the-human-vasculature/train/0006ff2aa7cd.tif\").read().transpose(1,2,0)\n# # image = image.reshape((512, 512))?\n# plt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_annotations():\n    annotations = []\n\n    with open(\"/kaggle/input/hubmap-hacking-the-human-vasculature/polygons.jsonl\", \"r\") as file:\n        for line in tqdm(file):\n            annotation = json.loads(line)\n            annotations.append(annotation)\n    return annotations\n\ndef get_instance_tuples(annotations):\n    image_tup = []\n\n    for annotation in annotations:\n        image_id = annotation[\"id\"]\n    #     image_ids.append(image_id)\n\n        for structure_annotation in annotation[\"annotations\"]:\n            image_path = f\"/kaggle/input/hubmap-hacking-the-human-vasculature/train/{image_id}.tif\"\n            structure_type = structure_annotation[\"type\"]\n    #         structure_types.append(structure_type)\n\n            coordinates = structure_annotation[\"coordinates\"]\n    #         coordinates_list.append(coordinates)\n            image_tup.append((image_path, structure_type, coordinates))\n    return image_tup\n\ndef get_semantic_dicts(annotations):\n    image_dict={}\n\n    for annotation in annotations:\n        image_id = annotation[\"id\"]\n        image_path = f\"/kaggle/input/hubmap-hacking-the-human-vasculature/train/{image_id}.tif\"\n        image_dict[image_id] = {'path':image_path, 'structure_types' : [], 'coordinates' : []}\n        for structure_annotation in annotation[\"annotations\"]:\n            structure_type = structure_annotation[\"type\"]\n            image_dict[image_id]['structure_types'].append(structure_type)\n            coordinates = structure_annotation[\"coordinates\"]\n            image_dict[image_id]['coordinates'].append(coordinates)\n    return image_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations = get_annotations()\nimage_dict = get_semantic_dicts(annotations)\nimage_tup = get_instance_tuples(annotations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.DataFrame(image_dict).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(all_data,test_size=0.2 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv(\"train_grouped.csv\", index=False)\nval_df.to_csv(\"val_grouped.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mim download mmdet --config mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco --dest ./checkpoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mmengine\n\nannotation = mmengine.load('/kaggle/input/hubmap-hacking-the-human-vasculature/polygons.jsonl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/hubmap-train-test/train_set.csv\")\nval = pd.read_csv(\"/kaggle/input/hubmap-train-test/val_set.csv\")\ntrain_gr = train.groupby('0').agg(lambda x: list(x))\nval_gr = val.groupby('0').agg(lambda x: list(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ninstance_seg = np.array(ast.literal_eval(train.iloc[0]['2']))[..., 1]\nclass_id_map = np.array(ast.literal_eval(train.iloc[0]['2']))[..., 0]\nclass_labels = np.unique(class_id_map)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"instance_seg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_id_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train.to_records(index=False).tolist()\n# val = val.to_records(index=False).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gr = train.groupby('0').agg(lambda x: list(x))\nval_gr = val.groupby('0').agg(lambda x: list(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(ast.literal_eval(train_gr.iloc[0]['2'][0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\nnp.array(ast.literal_eval(train_gr.iloc[0]['2'][0])[0]).transpose(1,0)\n# del px\n# del py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast \ndef get_coco(df, out_file):\n    type_dict = {'blood_vessel':0, 'glomerulus':1, 'unsure':2}\n    images = []\n    annotations = []\n    obj_count = 0\n    idx = 0\n    for i, row in df.iterrows():\n#         print(row)\n        file_path = i\n        filename = file_path.split('/')[-1]\n        images.append(dict(\n                    id=idx,\n                    file_name=filename,\n                    height=512,\n                    width=512))\n        idx += 1\n        bboxes = []\n        labels = []\n        masks = []\n        for type_, coords in zip(row['1'], row['2']):\n            if type_ == 'unsure':\n                continue\n            coords = ast.literal_eval(coords)\n            for coord in coords:\n                poly = [(x + 0.5, y + 0.5) for x, y in coord]\n                poly = [p for x in poly for p in x]\n                px, py = np.array(coord).transpose(1,0)\n                x_min, y_min, x_max, y_max = (\n                    min(px), min(py), max(px), max(py))\n\n\n                data_anno = dict(\n                    image_id=idx,\n                    id=obj_count,\n                    category_id=type_dict[type_],\n                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n                    area=(x_max - x_min) * (y_max - y_min),\n                    segmentation=[poly],\n                    iscrowd=0)\n                annotations.append(data_anno)\n                obj_count += 1\n\n    coco_format_json = dict(\n        images=images,\n        annotations=annotations,\n        categories=[{'id':0, 'name': 'blood_vessel'}, {'id':1, 'name': 'glomerulus'}])\n    mmengine.dump(coco_format_json, out_file)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/json_configs/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_coco(train_gr, '/kaggle/working/json_configs/train.json')\nget_coco(val_gr, '/kaggle/working/json_configs/val.json')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmengine import Config\ncfg = Config.fromfile('/kaggle/working/checkpoints/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco.py')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\npprint(cfg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/checkpoints/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from mmengine.runner import set_random_seed\n\n# Modify dataset classes and color\ncfg.metainfo = {\n    'classes': (0, 1),\n    'palette': [\n        (220, 20, 60),\n        (20, 220, 120)\n    ]\n}\n\n# Modify dataset type and path\ncfg.data_root = '' #'/kaggle/input/hubmap-hacking-the-human-vasculature/train'\n\ncfg.train_dataloader.dataset.ann_file = '/kaggle/working/json_configs/train.json'\ncfg.train_dataloader.dataset.data_root = cfg.data_root\ncfg.train_dataloader.dataset.data_prefix.img = '/kaggle/input/hubmap-hacking-the-human-vasculature/train'\ncfg.train_dataloader.dataset.metainfo = cfg.metainfo\n\ncfg.val_dataloader.dataset.ann_file = '/kaggle/working/json_configs/val.json'\ncfg.val_dataloader.dataset.data_root = cfg.data_root\ncfg.val_dataloader.dataset.data_prefix.img = '/kaggle/input/hubmap-hacking-the-human-vasculature/train'\ncfg.val_dataloader.dataset.metainfo = cfg.metainfo\n\ncfg.test_dataloader = cfg.val_dataloader\n\n# Modify metric config\ncfg.val_evaluator.ann_file = '/kaggle/working/json_configs/val.json'\ncfg.test_evaluator = cfg.val_evaluator\n\n# Modify num classes of the model in box head and mask head\n# cfg.model.roi_head.bbox_head.num_classes = 2\n# cfg.model.roi_head.mask_head.num_classes = 2\ncfg.num_classes = 2\ncfg.num_things_classes =2\n# We can still the pre-trained Mask RCNN model to obtain a higher performance\ncfg.load_from = '/kaggle/working/checkpoints/mask2former_swin-s-p4-w7-224_8xb2-lsj-50e_coco_20220504_001756-c9d0c4f2.pth'\n\n# Set up working dir to save files and logs.\ncfg.work_dir = '/kaggle/working/exps'\n\n\n# We can set the evaluation interval to reduce the evaluation times\ncfg.train_cfg.val_interval = 1\n# We can set the checkpoint saving interval to reduce the storage cost\ncfg.default_hooks.checkpoint.interval = 3\n\n# The original learning rate (LR) is set for 8-GPU training.\n# We divide it by 8 since we only use one GPU.\ncfg.optim_wrapper.optimizer.lr = 1e-3\ncfg.default_hooks.logger.interval = 10\n\n\n# Set seed thus the results are more reproducible\n# cfg.seed = 0\n# set_random_seed(0, deterministic=False)\n\n# We can also use tensorboard to log the training process\ncfg.visualizer.vis_backends.append({\"type\":'TensorboardVisBackend'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pprint(cfg.train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install requests==2.28.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from mmdet.datasets import build_dataset\n# from mmdet.models import build_detector\nfrom mmengine.runner import Runner\n\n# build the runner from config\nrunner = Runner.from_cfg(cfg)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"runner.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNET","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_tup, test_tup = train_test_split(image_tup,test_size=0.2 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_tup)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tile_meta = pd.read_csv(\"/kaggle/input/hubmap-hacking-the-human-vasculature/tile_meta.csv\")\n# train_images = os.listdir(\"/kaggle/input/hubmap-hacking-the-human-vasculature/train/\")\n# tile_meta_available = tile_meta[tile_meta['dataset'].isin([1,2])]\n# train_images_filtered = []\n# for img in train_images:\n#     img_name = img.split(\"/\")[-1].split(\".\")[0]\n# #     print(img_name)\n#     if img_name in tile_meta_available['id'].tolist():\n#         train_images_filtered.append(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for k, v in image_dict.items():\n#     image =  rasterio.open(v['path']).read()\n#     image_shape = image.shape[1:]\n#     mask = np.zeros(image_shape, dtype=np.uint8)\n#     pts_array = []\n#     for coord in v['coordinates']:\n#         pts = np.array(coord).reshape((-1, 1, 2)).astype(np.int32)\n#         pts_array.append(pts)\n#     cv2.fillPoly(mask, pts_array, 255)\n# #     mask = np.expand_dims(mask.astype(np.float32), axis=0)\n#     print(mask.shape)\n#     overlay = np.copy(image.transpose(1,2,0))\n#     overlay[mask > 0] = [1,0,0]  # Set the mask pixels to red (adjust color as needed)\n\n#     # Plot the image with the overlay\n#     fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n#     axes[0].imshow(image.transpose(1,2,0))\n#     axes[0].set_title('Image')\n#     axes[1].imshow(overlay)\n#     axes[1].set_title('Image with Mask Overlay')\n#     plt.show()\n#     break\n# #     np.expand_dims(mask.astype(np.float32), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for annotation in annotations:\n# #     print(annotation)\n# #     break\n#     id_ = annotation['id']\n#     image =  rasterio.open(f\"/kaggle/input/hubmap-hacking-the-human-vasculature/train/{id_}.tif\").read().transpose(1,2,0)\n#     for i in range(len(annotation[\"annotations\"])):\n#         structure_type = annotation[\"annotations\"][i][\"type\"]\n#         coordinates = annotation[\"annotations\"][i][\"coordinates\"]\n#         # Create a binary mask from the coordinates\n#         mask = np.zeros_like(image[..., 0], dtype=np.uint8)\n#         pts = np.array(coordinates).reshape((-1, 1, 2)).astype(np.int32)\n#         cv2.fillPoly(mask, [pts], 255)\n\n#         # Overlay the mask on the image\n#         overlay = cv2.addWeighted(image, 0.5, cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB), 0.5, 0)\n\n#         # Display the image with the overlay\n#         plt.imshow(overlay)\n#         plt.title(f\"Structure Type: {structure_type}\")\n#         plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_input = get_preprocessing_fn('timm-resnest101e', pretrained='imagenet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image =  rasterio.open(\"/kaggle/input/hubmap-hacking-the-human-vasculature/train/0006ff2aa7cd.tif\").read()\npreprocess_input(image.transpose(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from segmentation_models_pytorch.encoders import get_preprocessing_fn\nimport ast\nimport torch\nfrom torch.utils.data import Dataset\n\nclass CustomDataset_instance(Dataset):\n    def __init__(self, image_tup):\n        self.image_tup = image_tup\n        self.preprocess_input = get_preprocessing_fn('timm-resnest101e', pretrained='imagenet')\n\n    def __getitem__(self, index):\n        # Load image\n        image_path, type_, coords, = self.image_tup[index]\n        image = rasterio.open(image_path).read()#.transpose(1, 2, 0)\n        image = self.preprocess_input(image.transpose(1,2,0)).transpose(2,0,1)\n#         image = torch.from_numpy(image).float() / 255.0  # Normalize image to [0, 1]\n        type_dict = {'blood_vessel':1, 'glomerulus':2, 'unsure':3}\n        # Load corresponding annotation\n#         annotation = self.annotations[index]\n        mask = self._create_mask(ast.literal_eval(coords), image.shape[1:], type_dict[type_])\n\n        return {'image': image, 'mask': mask}\n\n    def __len__(self):\n        return len(self.image_tup)\n\n    def _create_mask(self, coordinates, image_shape, type_):\n        mask = np.zeros(image_shape, dtype=np.uint8)\n#         print(mask.shape)\n        pts = np.array(coordinates).reshape((-1, 1, 2)).astype(np.int32)\n        cv2.fillPoly(mask, [pts], type_)\n#         print((mask > 0).astype(np.uint8).shape)\n        return np.expand_dims(mask.astype(np.float32), axis=0)\n        \nclass CustomDataset_semantic(Dataset):\n    def __init__(self, image_dict):\n        self.image_dict = image_dict\n        self.keys = list(image_dict.keys())\n\n    def __getitem__(self, index):\n        # Load image\n        key = self.keys[index]\n        value = self.image_dict[key]\n        \n        image = rasterio.open(value['path']).read()#.transpose(1, 2, 0)\n        image = torch.from_numpy(image).float() / 255.0  # Normalize image to [0, 1]\n        image_shape = image.shape[1:]\n        mask = np.zeros(image_shape, dtype=np.uint8)\n        pts_array = []\n        for coord in (value['coordinates']):\n            pts = np.array(coord).reshape((-1, 1, 2)).astype(np.int32)\n            pts_array.append(pts)\n        cv2.fillPoly(mask, pts_array, 1)\n        mask = np.expand_dims(mask.astype(np.float32), axis=0)\n        return {'image': image, 'mask': mask}\n\n    def __len__(self):\n        return len(self.keys)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combined_\ntrain_dataset = CustomDataset_instance(train)\ntest_dataset = CustomDataset_instance(val)\n\n# dataset_2 = CustomDataset_semantic(image_dict)\n# dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\ndata = next(iter(dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(train_tup).to_csv(\"train_set.csv\", index=False)\npd.DataFrame(test_tup).to_csv(\"val_set.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = next(iter(dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['mask']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=0\nfor data in dataloader:\n    \n    image = data['image'].squeeze().numpy().transpose(1,2,0)\n    mask = data['mask'].squeeze().numpy()\n#     print()\n#     continue\n    arr = [0,0,0]\n    # Overlay the mask on the image\n    overlay = np.copy(image)\n    label = np.unique(mask)[np.nonzero(np.unique(mask))][0]\n    arr[int(label-1)] = 1\n    print(arr)\n    overlay[mask > 0] =  arr # Set the mask pixels to red (adjust color as needed)\n\n    # Plot the image with the overlay\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(image)\n    axes[0].set_title('Image')\n    axes[1].imshow(overlay)\n    axes[1].set_title('Image with Mask Overlay')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['image'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class UNet(nn.Module):\n#     def __init__(self, in_channels, out_channels):\n#         super(UNet, self).__init__()\n\n#         # Encoder\n#         self.encoder = nn.Sequential(\n#             nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n#             nn.ReLU(inplace=True),\n#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n#             nn.ReLU(inplace=True),\n#             nn.MaxPool2d(kernel_size=2, stride=2)\n#         )\n\n#         # Decoder\n#         self.decoder = nn.Sequential(\n#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n#             nn.ReLU(inplace=True),\n#             nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n#             nn.ReLU(inplace=True),\n#             nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n#         )\n\n#     def forward(self, x):\n#         # Encoder\n#         x = self.encoder(x)\n\n#         # Decoder\n#         x = self.decoder(x)\n\n#         return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset = torch.utils.data.random_split(dataset, [17000,518])\n_, pred_dataset = torch.utils.data.random_split(dataset, [,518])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model\n# del batch\n# del criterion\n# del masks\n# del images\n# del outputs\n# import gc\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    \n    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n    \n    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n    \n    return thresholded  #","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\n# Create an instance of the U-Net model\nin_channels = 3  # Modify this based on the number of input channels in your images\nout_channels = 1  # Modify this based on the number of classes or structures being segmented\nmodel = smp.UnetPlusPlus(\n    encoder_name=\"timm-resnest101e\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=4,                      # model output channels (number of classes in your dataset)\n)\nmodel = model.cuda()\n# Define the loss function (e.g., binary cross-entropy)\ncriterion = smp.losses.DiceLoss('multiclass')#nn.CrossEntropyLoss()#BCEWithLogitsLoss()\n\n# Define the optimizer (e.g., Adam)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    # Set the model to training mode\n    model.train()\n    pbar = tqdm(train_dataloader)\n    running_loss = 0.0\n    for i, batch in enumerate(pbar):\n        images = batch['image'].cuda()\n        masks = batch['mask'].cuda()\n\n        # Zero the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n\n        # Calculate the loss\n        loss = criterion(outputs, masks.long())\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(outputs, 1).unsqueeze(1).type(torch.int32), masks.type(torch.int32), mode='multiclass', num_classes = 3)\n\n        # then compute metrics with required reduction (see metric docs)\n        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n        running_loss += loss.item()\n        pbar.set_postfix({'loss':(running_loss/(i+1)), 'iou':iou_score})\n    # Print training loss for the epoch\n    epoch_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n\n    # Evaluation on the validation set\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            images = batch['image'].cuda()\n            masks = batch['mask'].cuda()\n\n            # Forward pass\n            outputs = model(images)\n\n            # Calculate the loss\n            loss = criterion(outputs, masks)\n            val_loss += loss.item()\n\n    # Print validation loss for the epoch\n    val_loss /= len(val_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb login\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nlr_monitor = LearningRateMonitor(logging_interval='step')\n# from pytorch_lightning.metrics.functional import accuracy\n\nclass Unetplusplus(pl.LightningModule):\n    def __init__(self, encoder_name, encoder_weights, in_channels, classes, learning_rate):\n        super(Unetplusplus, self).__init__()\n        self.model = smp.UnetPlusPlus(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels,\n            classes=classes\n        )\n        self.criterion = smp.losses.DiceLoss('multiclass')\n        self.learning_rate = learning_rate\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, masks = batch['image'].float(), batch['mask']\n        outputs = self(images)\n        loss = self.criterion(outputs, masks.long())\n        tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(outputs, 1).unsqueeze(1).type(torch.int32), masks.type(torch.int32), mode='multiclass', num_classes=3)\n        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n        self.log('train_loss', loss, prog_bar=True, logger=True)\n        self.log('train_iou', iou_score, prog_bar=True, logger=True)\n        del batch\n        gc.collect()\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, masks = batch['image'].float(), batch['mask']\n        outputs = self(images)\n        loss = self.criterion(outputs, masks.long())\n        self.log('val_loss', loss, prog_bar=True, logger=True)\n        return loss\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=0.2)\n\n        lr_scheduler_cont = {\n            'scheduler':  torch.optim.lr_scheduler.ReduceLROnPlateau(\n                                                                        optimizer,\n                                                                        mode='min',\n                                                                        factor=0.8,\n                                                                        patience=200,\n                                                                        threshold=1e-2,\n                                                                        min_lr=1e-8,\n                                                                        verbose=True\n                                                                    ),\n            'name': 'learning_rate',\n            'monitor' : 'train_loss',\n            \"interval\" : \"step\" \n        }\n        return optimizer\n\n# Create the Lightning DataModules\n# train_dataloader = DataLoader(...)\n# val_dataloader = DataLoader(...)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n# Create the Lightning trainer\nwandb_logger = WandbLogger()\ncheckpoint_callback = ModelCheckpoint(dirpath=\"/kaggle/working/ckpt/\", save_top_k=3, monitor=\"val_loss\")\ntrainer = pl.Trainer(max_epochs=15, devices=[0,1],accelerator=\"gpu\",precision=16, logger=wandb_logger, callbacks=[checkpoint_callback])  # Set the appropriate number of GPUs\n\n# Create the Lightning model\nmodel = Unetplusplus(\n    encoder_name=\"timm-resnest101e\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=4,\n    learning_rate=1e-3\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ = smp.UnetPlusPlus(\n    encoder_name=\"timm-resnest101e\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=4,                      # model output channels (number of classes in your dataset)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_.load_state_dict(torch.load('/kaggle/input/hubmap-unet-inst-model/model.ckpt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, train_dataloader, val_dataloader)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_checkpoint(\"model.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(),'/kaggle/working/model.pth' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.sum(torch.rand(4,1,512,512), dim=[1,2,3])/(512*512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Unetplusplus(pl.LightningModule):\n    def __init__(self, encoder_name, encoder_weights, in_channels, classes, learning_rate):\n        super(Unetplusplus, self).__init__()\n        self.model = smp.UnetPlusPlus(\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels,\n            classes=classes\n        )\n        self.criterion = smp.losses.DiceLoss('multiclass')\n        self.learning_rate = learning_rate\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, masks = batch['image'], batch['mask']\n        outputs = self(images)\n        loss = self.criterion(outputs, masks.long())\n        tp, fp, fn, tn = smp.metrics.get_stats(torch.argmax(outputs, 1).unsqueeze(1).type(torch.int32), masks.type(torch.int32), mode='multiclass', num_classes=3)\n        iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n        self.log('train_loss', loss, prog_bar=True, logger=True)\n        self.log('train_iou', iou_score, prog_bar=True, logger=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, masks = batch['image'], batch['mask']\n        outputs = self(images)\n        loss = self.criterion(outputs, masks.long())\n        self.log('val_loss', loss, prog_bar=True, logger=True)\n        return loss\n    def predict_step(self, batch, batch_idx):\n        images, masks = batch['image'], batch['mask']\n#         print(imah)\n        output = self(images)\n        return output\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Unetplusplus(\n    encoder_name=\"timm-resnest101e\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=4,\n    learning_rate=1e-4\n)\nmodel = model.load_from_checkpoint(checkpoint_path='/kaggle/input/hubmap-unet-inst-model/model.ckpt',\n                                    encoder_name=\"timm-resnest101e\",\n                                    encoder_weights=\"imagenet\",\n                                    in_channels=3,\n                                    classes=4,\n                                    learning_rate=1e-4)\nmodel.eval()\n\ntrainer = pl.Trainer(devices = [0],accelerator=\"gpu\", inference_mode=True)    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = trainer.predict(model, pred_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results[0][:,1,:,:].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.eval()\n# val_loss = 0.0\n# iou = 0.0\n# for i in [0.6]:\n#     with torch.no_grad():\nfor i,batch in enumerate(pred_dataloader):\n    images = batch['image'].cuda()\n    masks = batch['mask'].cuda()\n    outputs = results[i][:,2,::]\n#     # Forward pass\n#     outputs = model.predict(images)\n\n    # Calculate the loss\n#     loss = criterion(outputs, maskslong())\n#     val_loss += loss.item()\n    \n    sig_out = torch.sigmoid(outputs)\n    images = images.detach().cpu().squeeze().numpy().transpose(1,2,0)\n    masks = masks.detach().cpu().squeeze().numpy()\n    sig_out = sig_out.detach().cpu().squeeze().numpy()\n    overlay = np.copy(images)\n    overlay[masks > 0] = [1, 0, 0]  # Set the mask pixels to red (adjust color as needed)\n    overlay2 = np.copy(images)\n    overlay2[sig_out > 0.2] = [1, 0, 0]  # Set the mask pixels to red (adjust color as needed)\n\n    # Plot the image with the overlay\n    fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n    axes[0].imshow(images)\n    axes[0].set_title('Image')\n    axes[1].imshow(overlay)\n    axes[1].set_title('Image with Mask Overlay')\n    axes[2].imshow(overlay2)\n    axes[2].set_title('Image with pred Overlay')\n    plt.show()\n    if i>10:\n        break\n    #         print(sig_out[sig_out>0.5])\n    #         break\n#     tp, fp, fn, tn = smp.metrics.get_stats(outputs, masks.type(torch.int32).cuda(), mode='binary', threshold=i)\n#     iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\").detach().cpu().numpy()\n# #         print(f\"Score:{torch.sum(outputs, dim=[1,2,3])/torch.count_nonzero(outputs)}, iou: {iou_score}\")\n#     iou += iou_score\n# # Print validation loss for the epoch\n# val_loss /= len(pred_dataloader)\n# iou /= len(pred_dataloader)\n# print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, iou : {iou} , thrshold: {i}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = f\"/kaggle/input/hubmap-hacking-the-human-vasculature/test/72e40acccadf.tif\"\nimage = rasterio.open(image_path).read()\nmodel2 = model.cuda()\npred = model2.predict_step({'image':(torch.from_numpy(np.expand_dims(image,axis=0)).float() / 255.0).cuda(), 'mask':np.array([])}, 0)[:,1,::]\npred = pred.detach().cpu()\nsig_out = torch.sigmoid(pred).squeeze().numpy()\nprint(sig_out.shape)\noverlay = np.copy(image.transpose(1,2,0))\noverlay[sig_out>0.2] = [0, 255, 0]  # Set the mask pixels to red (adjust color as needed)\n\n#         # Plot the image with the overlay\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(image.transpose(1,2,0))\naxes[0].set_title('Image')\naxes[1].imshow(overlay)\naxes[1].set_title('Image with Mask Overlay')\n#         axes[2].imshow(overlay2)\n#         axes[2].set_title('Image with pred Overlay')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds=[]\nfor file in os.listdir(\"/kaggle/input/hubmap-hacking-the-human-vasculature/test/\"):\n    image_path = f\"/kaggle/input/hubmap-hacking-the-human-vasculature/test/{file}\"\n    image = rasterio.open(image_path).read()\n    pred = torch.sigmoid(model(torch.from_numpy(np.expand_dims(image.astype(np.float32),axis=0)).cuda()))\n    pred = pred.detach().cpu().squeeze().numpy()\n    pred = encode_binary\n    preds.append(pred)\n# overlay = np.copy(image.transpose(1,2,0))\n# overlay[pred > 0.3] = [1, 0, 0]  # Set the mask pixels to red (adjust color as needed)\n\n# #         # Plot the image with the overlay\n# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n# axes[0].imshow(image.transpose(1,2,0))\n# axes[0].set_title('Image')\n# axes[1].imshow(overlay)\n# axes[1].set_title('Image with Mask Overlay')\n# #         axes[2].imshow(overlay2)\n# #         axes[2].set_title('Image with pred Overlay')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import base64\nimport numpy as np\nfrom pycocotools import _mask as coco_mask\nimport typing as t\nimport zlib\n\n\ndef encode_binary_mask(mask: np.ndarray) -> t.Text:\n    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n\n    # check input mask --\n    if mask.dtype != np.bool:\n        raise ValueError(\n            \"encode_binary_mask expects a binary mask, received dtype == %s\" %\n            mask.dtype)\n\n    mask = np.squeeze(mask)\n    if len(mask.shape) != 2:\n        raise ValueError(\n            \"encode_binary_mask expects a 2d mask, received shape == %s\" %\n            mask.shape)\n\n    # convert input mask to expected COCO API input --\n    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n    mask_to_encode = mask_to_encode.astype(np.uint8)\n    mask_to_encode = np.asfortranarray(mask_to_encode)\n\n    # RLE encode mask --\n    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n\n    # compress and base64 encoding --\n    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n    base64_str = base64.b64encode(binary_str)\n    return base64_str","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mask2former","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}